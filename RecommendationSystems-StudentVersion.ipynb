{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System rekomendujący na bazie Netflix Prize award\n",
    "\n",
    "1. Zacznijmy od obejrzenia wykładu [rozdział 9](http://www.mmds.org/)\n",
    "2. proszę ściągnac bazę netflixa o której była mowa w/w wykładzie [z kaggle](https://www.kaggle.com/netflix-inc/netflix-prize-data) (2GB) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaczniemy od wczytania danych, ponizszy listing wczytuje dane z jednego pliku i robi z nich trójkę (user, product, rating). Wykorzystamy do tego predefiniowany obiekt Rating z mlib.recommendation (prosze zwrócic uwagę że w tej konwencji nasz film będzie produktem)\n",
    "\n",
    "#### Zadanie 1:\n",
    "zmodyfikuj poniższy listing tak aby wczytywać wsystkie pięc plików"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File './RS/combined_data_1.txt' loaded!\n",
      "Finished! Processed lines: 24053764\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.mllib.recommendation import Rating\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# tutaj zaincjalizujemy klaster (na jednym komputerze) sparka\n",
    "# zmodyfikujemy nieco std ustawienia maszyny java zwiększając domyslne (bardzo małe) limity pamieci\n",
    "# local[*] oznacza że użyjemy wsystkich rdzeni - jeśli zabrankie nam ramu możemy zmniejszyć ilość tą ilość\n",
    "# polecam spoglądać do konsoli na http://localhost:4040/ aby monitorować zużycie zasobów\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName(\"recommendation\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '4G')\n",
    "        .set('spark.driver.memory', '20G')\n",
    "        .set('spark.driver.maxResultSize', '10G'))\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "#files = ['./RS/combined_data_1.txt',\n",
    "        #'./RS/combined_data_2.txt',\n",
    "        #'./RS/combined_data_3.txt',\n",
    "        #'./RS/combined_data_4.txt'] # Ufam, że użytkownik wprowadził nazwy plików, które istnieją...\n",
    "files = [ './RS/combined_data_1.txt' ]\n",
    "\n",
    "ratings = []\n",
    "for single_name in files:\n",
    "    f = open(single_name)\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        if line.endswith(':'):\n",
    "            movie_id = int(line[:-1])        \n",
    "        else:\n",
    "            user_id, rating, _ = line.split(',')\n",
    "            r = Rating(int(user_id), int(movie_id), int(rating))\n",
    "            ratings.append(r)\n",
    "    f.close()\n",
    "    print(\"File '\" + single_name + \"' loaded!\")\n",
    "\n",
    "print('Finished! Processed lines: ' + str(len(ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "#tutaj wczytamy identyfikatory filmów i ich tytuły\n",
    "\n",
    "f = open('./RS/movie_titles.csv', encoding = \"ISO-8859-1\")\n",
    "g = [l.strip().split(',') for l in f.readlines()]\n",
    "id2title = {int(a[0]):','.join(a[2:]) for a in g}\n",
    "f.close()\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "id2title\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! Elapsed time: 31.34 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# utworzmy z naszej listy ratingow zasob rdd (rozproszenie)\n",
    "start = time.time()\n",
    "ratings_rdd = sc.parallelize(ratings)\n",
    "end = time.time()\n",
    "print('Finished! Elapsed time: ' + str(round(end - start, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 2\n",
    "Przefiltruj ratings_rdd aby wziac pod uwage filmy ktory maja co najmniej 50 ocen\n",
    "tip: zrób napierw liste par (movie_id, Rating) i pogrupuj ją przy użyciu GroupByKey a nstępnie przefiltruj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "ratings_pairs = ratings_rdd.map(lambda r: (r.product, r))\n",
    "rg = ratings_pairs.groupByKey() # ???\n",
    "\n",
    "# ???\n",
    "ratings_filtered = rg.filter(lambda r: len(r[1]) >= 50).collect() # ???\n",
    "\n",
    "\n",
    "# zasob rdd na przefiltrowanym obiekcie\n",
    "ratings_filtered = sc.parallelize(ratings_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 - finished!\n",
      "Stage 2 - finished!\n",
      "Stage 3 - finished!\n",
      "Stage 4 - finished!\n",
      "Stage 5 - finished!\n",
      "Stage 6 - finished!\n",
      "Finished! Elapsed time: 203.6 seconds.\n"
     ]
    }
   ],
   "source": [
    "# TROCHĘ INNYM SPOSOBEM - na około (zmiana na DataFrame po to by korzystać z SQL Queries)\n",
    "import pyspark.sql.functions as ps_functions\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "data = SQLContext(sc)\n",
    "print('Stage 1 - finished!')\n",
    "\n",
    "all_ratings = data.createDataFrame(ratings_rdd)\n",
    "print('Stage 2 - finished!')\n",
    "\n",
    "fragment = Window.partitionBy('product')\n",
    "print('Stage 3 - finished!')\n",
    "\n",
    "query = all_ratings.select('user', 'product', 'rating', ps_functions.count('product').over(fragment).alias(\"ratings_count\"))\n",
    "print('Stage 4 - finished!')\n",
    "\n",
    "marker = query.filter(\"ratings_count >= 50\").select('user','product','rating')\n",
    "print('Stage 5 - finished!')\n",
    "\n",
    "ratings = sc.parallelize(marker.collect())\n",
    "print('Stage 6 - finished!')\n",
    "\n",
    "end = time.time()\n",
    "print('Finished! Elapsed time: ' + str(round(end - start, 2)) + ' seconds.')\n",
    "# TROCHĘ INNYM SPOSOBEM - na około (zmiana na DataFrame po to by korzystać z SQL Queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokonamy teraz faktoryzacji macierzy, nasej utility Matrix (zasób RDD ratings_filtered jest własnie taką macierzą) przy użyciu aproksymacji algorytmu spadku gradientowego\n",
    "![alt](https://edersoncorbari.github.io/assets/images/blog/als-matrix-rec-calc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! Elapsed time: 79.81 seconds.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import time\n",
    "\n",
    "rank = 10\n",
    "numIterations = 10\n",
    "\n",
    "start = time.time()\n",
    "model = ALS.train(ratings, rank, numIterations)\n",
    "end = time.time()\n",
    "print('Finished! Elapsed time: ' + str(round(end - start, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zrzucilismy uzytkowników i filmy na nowy wymiar o wielkości 10. Możemy wprost poprosić o te macierze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "users  = model.userFeatures()\n",
    "movies = model.productFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gęsta ale 'wąska' macierz movies zmieści się nam już bezproblemowo w RAM, a zatem zróbmy z niej po prostu macierz numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "movies_mtx = np.array(movies.map(lambda rv:rv[1]).collect())\n",
    "movie2row_number = movies.map(lambda rv:rv[0]).collect() #movie id to row number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zróbmy prosty ekesperyment, zobaczmy czy jakie filmy są 'podobne' do 'Władcy pierścieni'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tytuł: The Lord of the Rings\n",
      "[-0.4527832  -0.08132     0.23505069 -0.54495251  0.38770333  0.32258755\n",
      " -0.23984745  0.03348436  0.52501208  0.24838813]\n"
     ]
    }
   ],
   "source": [
    "id2title[1757] # 'The Lord of the Rings'\n",
    "\n",
    "lotr_idx = movie2row_number.index(1757)\n",
    "lotr_vector = movies_mtx[lotr_idx]\n",
    "print('Tytuł: ' + id2title[1757])\n",
    "print(lotr_vector) #to jest wladca pierscienie rzutowany na latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odległości: [1561 4481 2720 3569 2072 4108 2782 1176  313 2449]\n",
      "Tytuł filmu o ID 1561: The Lord of the Rings\n",
      "Tytuł filmu o ID 4481: Escaflowne: The Movie\n",
      "Tytuł filmu o ID 2720: Star Trek: The Motion Picture\n",
      "Tytuł filmu o ID 3569: The Lawnmower Man\n",
      "Tytuł filmu o ID 2072: The Clan of the Cave Bear\n",
      "Tytuł filmu o ID 4108: Burn Up W\n",
      "Tytuł filmu o ID 2782: The Return of the King\n",
      "Tytuł filmu o ID 1176: Cherry 2000\n",
      "Tytuł filmu o ID 313: Journey to the Center of the Earth\n",
      "Tytuł filmu o ID 2449: Logan's Run\n"
     ]
    }
   ],
   "source": [
    "# obliczmy macierz odleglosci \n",
    "from scipy.spatial import distance\n",
    "\n",
    "ds = distance.cdist([lotr_vector], movies_mtx, 'cosine')[0]\n",
    "dist = ds.argsort()[:10] # 10 - liczba filmów\n",
    "\n",
    "print('Odległości: ' + str(dist))\n",
    "\n",
    "for i in dist:\n",
    "    print('Tytuł filmu o ID ' + str(i) + ': ' + id2title[movie2row_number[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wartość odległości: 0.0 dotyczy filmu: The Lord of the Rings\n",
      "Wartość odległości: 0.07878517057753731 dotyczy filmu: Escaflowne: The Movie\n",
      "Wartość odległości: 0.08781767506452776 dotyczy filmu: Star Trek: The Motion Picture\n",
      "Wartość odległości: 0.08937511326538616 dotyczy filmu: The Lawnmower Man\n",
      "Wartość odległości: 0.09714696540534362 dotyczy filmu: The Clan of the Cave Bear\n",
      "Wartość odległości: 0.09870806179188363 dotyczy filmu: Burn Up W\n",
      "Wartość odległości: 0.09989895667984683 dotyczy filmu: The Return of the King\n",
      "Wartość odległości: 0.1067716968402107 dotyczy filmu: Cherry 2000\n",
      "Wartość odległości: 0.10821727445284046 dotyczy filmu: Journey to the Center of the Earth\n",
      "Wartość odległości: 0.10967764047823081 dotyczy filmu: Logan's Run\n"
     ]
    }
   ],
   "source": [
    "for i in dist:\n",
    "    print('Wartość odległości: ' + str(ds[i]) + \" dotyczy filmu: \" + id2title[movie2row_number[i]])\n",
    "\n",
    "# print(str(ds[2018]) + ': ' + id2title[movie2row_number[2018]])\n",
    "# print(str(ds[928]) + ': ' + id2title[movie2row_number[928]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odległosć 0.0 to oczywiscie ten sam obiekt a zatem interesuje nas drugi w kolejności wpis. I cóż za niespodzianka? ludzie którzy ocenili wysoko Władce pierścieni ocenili również wysokos Powrót Króla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na ile cech powinniśmy faktoryzować Utility Matrix? \n",
    "\n",
    "#### Zadanie 3\n",
    "Dobierz paramter rank (ilość cech) dla obiektu ALS dzieląc zbiór ratings_filtered na 80% vs 20% (treningowy i testowy, tak jak na wykłdzie). Następnie wykorzystać metodę ALS.recommendProductsForUsers aby polecić filmy i zweryfikuj ile filmów udało Ci sie prawidłowo polecić ze zbioru testowego. Odpowiednio modyfikuj parametr rank aby otrzymać względnie wysoki wynik rekomendacji jednocześnie utrymując w miare wąskie macierze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.mllib.recommendation import Rating\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName(\"recommendation\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '4G')\n",
    "        .set('spark.driver.memory', '20G')\n",
    "        .set('spark.driver.maxResultSize', '10G'))\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "#files = ['./RS/combined_data_1.txt',\n",
    "        #'./RS/combined_data_2.txt',\n",
    "        #'./RS/combined_data_3.txt',\n",
    "        #'./RS/combined_data_4.txt'] # Ufam, że użytkownik wprowadził nazwy plików, które istnieją...\n",
    "files = [ './RS/combined_data_1.txt' ]\n",
    "\n",
    "ratings = []\n",
    "for single_name in files:\n",
    "    f = open(single_name)\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        if line.endswith(':'):\n",
    "            movie_id = int(line[:-1])        \n",
    "        else:\n",
    "            user_id, rating, _ = line.split(',')\n",
    "            r = Rating(int(user_id), int(movie_id), int(rating))\n",
    "            ratings.append(r)\n",
    "    f.close()\n",
    "    print(\"File '\" + single_name + \"' loaded!\")\n",
    "\n",
    "print('Loading finished! Processed lines: ' + str(len(ratings)))\n",
    "\n",
    "f = open('./RS/movie_titles.csv', encoding = \"ISO-8859-1\")\n",
    "g = [l.strip().split(',') for l in f.readlines()]\n",
    "id2title = {int(a[0]):','.join(a[2:]) for a in g}\n",
    "f.close()\n",
    "print('IDs and titles loaded!!')\n",
    "\n",
    "start = time.time()\n",
    "ratings_rdd = sc.parallelize(ratings)\n",
    "end = time.time()\n",
    "print('RDD resource created! Elapsed time: ' + str(round(end - start, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 - finished!\n",
      "Stage 2 - finished!\n",
      "Stage 3 - finished!\n",
      "Stage 4 - finished!\n",
      "Stage 5 - finished!\n",
      "Stage 6 - finished!\n",
      "Finished! Elapsed time: 167.79 seconds.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as ps_functions\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "data = SQLContext(sc)\n",
    "print('Stage 1 - finished!')\n",
    "\n",
    "all_ratings = data.createDataFrame(ratings_rdd)\n",
    "print('Stage 2 - finished!')\n",
    "\n",
    "fragment = Window.partitionBy('product')\n",
    "print('Stage 3 - finished!')\n",
    "\n",
    "query = all_ratings.select('user', 'product', 'rating', ps_functions.count('product').over(fragment).alias(\"ratings_count\"))\n",
    "print('Stage 4 - finished!')\n",
    "\n",
    "marker = query.filter(\"ratings_count >= 50\").select('user','product','rating')\n",
    "print('Stage 5 - finished!')\n",
    "\n",
    "ratings = sc.parallelize(marker.collect())\n",
    "print('Stage 6 - finished!')\n",
    "\n",
    "end = time.time()\n",
    "print('Finished! Elapsed time: ' + str(round(end - start, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = ratings.randomSplit([6,2,2], seed=0)\n",
    "\n",
    "emptyTest = test.map(lambda r: (r[0], r[1]))\n",
    "\n",
    "emptyValidation = validate.map(lambda r: (r[0], r[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 4\n",
    "Zrób program \"co obejrzeć dziś wieczorem\".\n",
    "Program powinien pokazaywać 5 tytułow, uzytkownik wskazuje jeden lub kilka z nich i system pokazuje kolejne rekomendacje. Każda kolejna rekomendacja powinna wskazywać coraz lepsze rekomendacje. \n",
    "\n",
    "problem można rozwiazać na kilka sposób. Można pogrupować przestrzeń filmów (macierz P.T) i pokazywać z różnych np. centroidy klastrów (które będą reprezentować gatunki filmów (prawdopodobnie) ). \n",
    "\n",
    "każde kolejne wybranie filmu będzie wymagało stworzenie nowego sztuznego użytkownika z ratingiem. W każdej iteracji takiego użytkownika należy rzucic na macierz Q (czyli na latentną macierz użytkownika). Przypatrz się dokładnie sposobowi mnożenia na obrazku powyżej i zastanów się jak otrzymać takiego użytkownika w przestrzeni cech?). Przypominam że macierz P.T jest nieodracalna ale można wykorzystać pseudo odwrotnosć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
