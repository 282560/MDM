{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System rekomendujący na bazie Netflix Prize award\n",
    "\n",
    "1. Zacznijmy od obejrzenia wykładu [rozdział 9](http://www.mmds.org/)\n",
    "2. proszę ściągnac bazę netflixa o której była mowa w/w wykładzie [z kaggle](https://www.kaggle.com/netflix-inc/netflix-prize-data) (2GB) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaczniemy od wczytania danych, ponizszy listing wczytuje dane z jednego pliku i robi z nich trójkę (user, product, rating). Wykorzystamy do tego predefiniowany obiekt Rating z mlib.recommendation (prosze zwrócic uwagę że w tej konwencji nasz film będzie produktem)\n",
    "\n",
    "#### Zadanie 1:\n",
    "zmodyfikuj poniższy listing tak aby wczytywać wsystkie pięc plików"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File './RS/combined_data_1.txt' loaded!\n",
      "Finished! Processed lines: 24053764\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.mllib.recommendation import Rating\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# tutaj zaincjalizujemy klaster (na jednym komputerze) sparka\n",
    "# zmodyfikujemy nieco std ustawienia maszyny java zwiększając domyslne (bardzo małe) limity pamieci\n",
    "# local[*] oznacza że użyjemy wsystkich rdzeni - jeśli zabrankie nam ramu możemy zmniejszyć ilość tą ilość\n",
    "# polecam spoglądać do konsoli na http://localhost:4040/ aby monitorować zużycie zasobów\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName(\"recommendation\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '4G')\n",
    "        .set('spark.driver.memory', '20G')\n",
    "        .set('spark.driver.maxResultSize', '10G'))\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "#files = ['./RS/combined_data_1.txt',\n",
    "        #'./RS/combined_data_2.txt',\n",
    "        #'./RS/combined_data_3.txt',\n",
    "        #'./RS/combined_data_4.txt'] # Ufam, że użytkownik wprowadził nazwy plików, które istnieją...\n",
    "files = [ './RS/combined_data_1.txt' ]\n",
    "\n",
    "ratings = []\n",
    "for single_name in files:\n",
    "    f = open(single_name)\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        if line.endswith(':'):\n",
    "            movie_id = int(line[:-1])        \n",
    "        else:\n",
    "            user_id, rating, _ = line.split(',')\n",
    "            r = Rating(int(user_id), int(movie_id), int(rating))\n",
    "            ratings.append(r)\n",
    "    f.close()\n",
    "    print(\"File '\" + single_name + \"' loaded!\")\n",
    "\n",
    "print('Finished! Processed lines: ' + str(len(ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "#tutaj wczytamy identyfikatory filmów i ich tytuły\n",
    "\n",
    "f = open('./RS/movie_titles.csv', encoding = \"ISO-8859-1\")\n",
    "g = [l.strip().split(',') for l in f.readlines()]\n",
    "id2title = {int(a[0]):','.join(a[2:]) for a in g}\n",
    "f.close()\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "id2title\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! Elapsed time: 31.56 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# utworzmy z naszej listy ratingow zasob rdd (rozproszenie)\n",
    "start = time.time()\n",
    "ratings_rdd = sc.parallelize(ratings)\n",
    "end = time.time()\n",
    "print('Finished! Elapsed time: ' + str(round(end - start, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 2\n",
    "Przefiltruj ratings_rdd aby wziac pod uwage filmy ktory maja co najmniej 50 ocen\n",
    "tip: zrób napierw liste par (movie_id, Rating) i pogrupuj ją przy użyciu GroupByKey a nstępnie przefiltruj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "ratings_pairs = ratings_rdd.map(lambda r: (r.product, r))\n",
    "rg = ratings_pairs.groupByKey() # ???\n",
    "\n",
    "# ???\n",
    "ratings_filtered = rg.filter(lambda r: len(r[1]) >= 50).collect() # ???\n",
    "\n",
    "\n",
    "# zasob rdd na przefiltrowanym obiekcie\n",
    "ratings_filtered = sc.parallelize(ratings_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 - finished!\n",
      "Stage 2 - finished!\n",
      "Stage 3 - finished!\n",
      "Stage 4 - finished!\n",
      "Stage 5 - finished!\n",
      "Stage 6 - finished!\n",
      "Finished! Elapsed time: 168.47 seconds.\n"
     ]
    }
   ],
   "source": [
    "# TROCHĘ INNYM SPOSOBEM - na około (zmiana na DataFrame po to by korzystać z SQL Queries)\n",
    "import pyspark.sql.functions as ps_functions\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "data = SQLContext(sc)\n",
    "print('Stage 1 - finished!')\n",
    "\n",
    "all_ratings = data.createDataFrame(ratings_rdd)\n",
    "print('Stage 2 - finished!')\n",
    "\n",
    "fragment = Window.partitionBy('product')\n",
    "print('Stage 3 - finished!')\n",
    "\n",
    "query = all_ratings.select('user', 'product', 'rating', ps_functions.count('product').over(fragment).alias(\"ratings_count\"))\n",
    "print('Stage 4 - finished!')\n",
    "\n",
    "marker = query.filter(\"ratings_count >= 50\").select('user','product','rating')\n",
    "print('Stage 5 - finished!')\n",
    "\n",
    "ratings = sc.parallelize(marker.collect())\n",
    "print('Stage 6 - finished!')\n",
    "\n",
    "end = time.time()\n",
    "print('Finished! Elapsed time: ' + str(round(end - start, 2)) + ' seconds.')\n",
    "# TROCHĘ INNYM SPOSOBEM - na około (zmiana na DataFrame po to by korzystać z SQL Queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokonamy teraz faktoryzacji macierzy, nasej utility Matrix (zasób RDD ratings_filtered jest własnie taką macierzą) przy użyciu aproksymacji algorytmu spadku gradientowego\n",
    "![alt](https://edersoncorbari.github.io/assets/images/blog/als-matrix-rec-calc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! Elapsed time: 80.86 seconds.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import time\n",
    "\n",
    "rank = 10\n",
    "numIterations = 10\n",
    "\n",
    "start = time.time()\n",
    "model = ALS.train(ratings, rank, numIterations)\n",
    "end = time.time()\n",
    "print('Finished! Elapsed time: ' + str(round(end - start, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zrzucilismy uzytkowników i filmy na nowy wymiar o wielkości 10. Możemy wprost poprosić o te macierze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "users  = model.userFeatures()\n",
    "movies = model.productFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gęsta ale 'wąska' macierz movies zmieści się nam już bezproblemowo w RAM, a zatem zróbmy z niej po prostu macierz numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "movies_mtx = np.array(movies.map(lambda rv:rv[1]).collect())\n",
    "movie2row_number = movies.map(lambda rv:rv[0]).collect() #movie id to row number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zróbmy prosty ekesperyment, zobaczmy czy jakie filmy są 'podobne' do 'Władcy pierścieni'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Lord of the Rings\n",
      "[-0.22731483  0.16489471 -0.4131501  -0.38900298 -0.48030087  0.04403076\n",
      " -0.03553521 -0.34933752  0.33792284  0.77826786]\n"
     ]
    }
   ],
   "source": [
    "id2title[1757] # 'The Lord of the Rings'\n",
    "\n",
    "lotr_idx = movie2row_number.index(1757)\n",
    "lotr_vector = movies_mtx[lotr_idx]\n",
    "print('Title: ' + id2title[1757])\n",
    "print(lotr_vector) #to jest wladca pierscienie rzutowany na latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: [1561  174  844  141 3245 2782 1957 3005 1147 4302]\n",
      "Movie title using ID 1561: The Lord of the Rings\n",
      "Movie title using ID 174: Todd McFarlane's Spawn\n",
      "Movie title using ID 844: Shaka Zulu\n",
      "Movie title using ID 141: Knights of Ramune\n",
      "Movie title using ID 3245: Cosplay Complex\n",
      "Movie title using ID 2782: The Return of the King\n",
      "Movie title using ID 1957: Mezzo\n",
      "Movie title using ID 3005: A Million to Juan\n",
      "Movie title using ID 1147: Complete Shamanic Princess\n",
      "Movie title using ID 4302: Hercules: The Legendary Journeys: Season 2\n"
     ]
    }
   ],
   "source": [
    "# obliczmy macierz odleglosci \n",
    "from scipy.spatial import distance\n",
    "\n",
    "ds = distance.cdist([lotr_vector], movies_mtx, 'cosine')[0]\n",
    "dist = ds.argsort()[:10] # 10 - liczba filmów\n",
    "\n",
    "print('Distance: ' + str(dist))\n",
    "\n",
    "for i in dist:\n",
    "    print('Movie title using ID ' + str(i) + ': ' + id2title[movie2row_number[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance value: 0.0 refers movie: The Lord of the Rings\n",
      "Distance value: 0.055450248493897236 refers movie: Todd McFarlane's Spawn\n",
      "Distance value: 0.05566620568613523 refers movie: Shaka Zulu\n",
      "Distance value: 0.0593692100299571 refers movie: Knights of Ramune\n",
      "Distance value: 0.07500270642058404 refers movie: Cosplay Complex\n",
      "Distance value: 0.08188804371612912 refers movie: The Return of the King\n",
      "Distance value: 0.08383961407035634 refers movie: Mezzo\n",
      "Distance value: 0.08502933677577662 refers movie: A Million to Juan\n",
      "Distance value: 0.09167598711851499 refers movie: Complete Shamanic Princess\n",
      "Distance value: 0.10279860640752103 refers movie: Hercules: The Legendary Journeys: Season 2\n"
     ]
    }
   ],
   "source": [
    "for i in dist:\n",
    "    print('Distance value: ' + str(ds[i]) + \" refers movie: \" + id2title[movie2row_number[i]])\n",
    "\n",
    "# print(str(ds[2018]) + ': ' + id2title[movie2row_number[2018]])\n",
    "# print(str(ds[928]) + ': ' + id2title[movie2row_number[928]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odległosć 0.0 to oczywiscie ten sam obiekt a zatem interesuje nas drugi w kolejności wpis. I cóż za niespodzianka? ludzie którzy ocenili wysoko Władce pierścieni ocenili również wysokos Powrót Króla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na ile cech powinniśmy faktoryzować Utility Matrix? \n",
    "\n",
    "#### Zadanie 3\n",
    "Dobierz paramter rank (ilość cech) dla obiektu ALS dzieląc zbiór ratings_filtered na 80% vs 20% (treningowy i testowy, tak jak na wykłdzie). Następnie wykorzystać metodę ALS.recommendProductsForUsers aby polecić filmy i zweryfikuj ile filmów udało Ci sie prawidłowo polecić ze zbioru testowego. Odpowiednio modyfikuj parametr rank aby otrzymać względnie wysoki wynik rekomendacji jednocześnie utrymując w miare wąskie macierze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File './RS/combined_data_1.txt' loaded!\n",
      "Loading finished! Processed lines: 24053764\n",
      "IDs and titles loaded!!\n",
      "RDD resource created! Elapsed time: 30.99 seconds.\n"
     ]
    }
   ],
   "source": [
    "#files = ['./RS/combined_data_1.txt',\n",
    "        #'./RS/combined_data_2.txt',\n",
    "        #'./RS/combined_data_3.txt',\n",
    "        #'./RS/combined_data_4.txt'] # Ufam, że użytkownik wprowadził nazwy plików, które istnieją...\n",
    "files = [ './RS/combined_data_1.txt' ]\n",
    "\n",
    "ratings = []\n",
    "for single_name in files:\n",
    "    f = open(single_name)\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        if line.endswith(':'):\n",
    "            movie_id = int(line[:-1])        \n",
    "        else:\n",
    "            user_id, rating, _ = line.split(',')\n",
    "            r = Rating(int(user_id), int(movie_id), int(rating))\n",
    "            ratings.append(r)\n",
    "    f.close()\n",
    "    print(\"File '\" + single_name + \"' loaded!\")\n",
    "\n",
    "print('Loading finished! Processed lines: ' + str(len(ratings)))\n",
    "\n",
    "f = open('./RS/movie_titles.csv', encoding = \"ISO-8859-1\")\n",
    "g = [l.strip().split(',') for l in f.readlines()]\n",
    "id2title = {int(a[0]):','.join(a[2:]) for a in g}\n",
    "f.close()\n",
    "print('IDs and titles loaded!!')\n",
    "\n",
    "start = time.time()\n",
    "ratings_rdd = sc.parallelize(ratings)\n",
    "end = time.time()\n",
    "print('RDD resource created! Elapsed time: ' + str(round(end - start, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 - finished!\n",
      "Stage 2 - finished!\n",
      "Stage 3 - finished!\n",
      "Stage 4 - finished!\n",
      "Stage 5 - finished!\n",
      "Stage 6 - finished!\n",
      "Finished! Elapsed time: 205.19 seconds.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as ps_functions\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "data = SQLContext(sc)\n",
    "print('Stage 1 - finished!')\n",
    "\n",
    "all_ratings = data.createDataFrame(ratings_rdd) # Tu zawsze pojawia się error przy pierwszym uruchomieniu. Drugie uruchomienie już go nie wywołuje\n",
    "print('Stage 2 - finished!')\n",
    "\n",
    "fragment = Window.partitionBy('product')\n",
    "print('Stage 3 - finished!')\n",
    "\n",
    "query = all_ratings.select('user', 'product', 'rating', ps_functions.count('product').over(fragment).alias(\"ratings_count\"))\n",
    "print('Stage 4 - finished!')\n",
    "\n",
    "marker = query.filter(\"ratings_count >= 50\").select('user','product','rating')\n",
    "print('Stage 5 - finished!')\n",
    "\n",
    "ratings = sc.parallelize(marker.collect())\n",
    "print('Stage 6 - finished!')\n",
    "\n",
    "end = time.time()\n",
    "print('Finished! Elapsed time: ' + str(round(end - start, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n",
      "Training completed for rank: 1/20. Elapsed time: 207.87 seconds.\n",
      "Training completed for rank: 2/20. Elapsed time: 185.76 seconds.\n",
      "Training completed for rank: 3/20. Elapsed time: 193.15 seconds.\n",
      "Training completed for rank: 4/20. Elapsed time: 187.55 seconds.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 4780.0 failed 1 times, most recent failure: Lost task 5.0 in stage 4780.0 (TID 3734, localhost, executor driver): java.net.SocketException: Software caused connection abort: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:212)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Software caused connection abort: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:212)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-eb9e78ac02b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memptyValid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mrates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mnewError\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0merrorsList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mind\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[1;36m2.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \"\"\"\n\u001b[1;32m-> 1202\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mstats\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mleft_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright_counter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1065\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    814\u001b[0m         \"\"\"\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 816\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    817\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 4780.0 failed 1 times, most recent failure: Lost task 5.0 in stage 4780.0 (TID 3734, localhost, executor driver): java.net.SocketException: Software caused connection abort: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:212)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Software caused connection abort: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:212)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "############################################################\n",
    "\n",
    "train, valid, test = ratings.randomSplit([6,2,2], seed=0)\n",
    "emptyTest = test.map(lambda r: (r[0], r[1]))\n",
    "emptyValid = valid.map(lambda r: (r[0], r[1]))\n",
    "print('Finished!')\n",
    "\n",
    "############################################################\n",
    "\n",
    "ranks = range(1, 21)\n",
    "\n",
    "errorsList = []\n",
    "models = []\n",
    "for i in range(len(ranks)):\n",
    "    errorsList.append(0)\n",
    "ind = 0\n",
    "\n",
    "remembered_error = float('inf')\n",
    "the_best_rank = -1\n",
    "for rank in ranks:\n",
    "    start = time.time()\n",
    "    model = ALS.train(train, rank, seed=5, iterations=20, lambda_=0.1)\n",
    "    pred = model.predictAll(emptyValid).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates = valid.map(lambda r: ((r[0], r[1]), r[2])).join(pred)\n",
    "    newError = math.sqrt(rates.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    errorsList[ind] = newError\n",
    "    ind += 1\n",
    "    if newError < remembered_error:\n",
    "        remembered_error = newError\n",
    "        the_best_rank = rank\n",
    "    end = time.time()\n",
    "    print('Training completed for rank: ' + str(rank) + '/' + str(len(ranks)) + '. Elapsed time: ' + str(round(end - start, 2)) + ' seconds.')\n",
    "print(\"The best rank %s\" % the_best_rank)\n",
    "\n",
    "############################################################\n",
    "\n",
    "train, test = ratings.randomSplit([8,2], seed=0)\n",
    "emptyTest = test.map(lambda r: (r[0], r[1]))\n",
    "\n",
    "############################################################\n",
    "\n",
    "start = time.time()\n",
    "model = ALS.train(train, the_best_rank, seed=5, iterations=20, lambda_=0.1)\n",
    "pred = model.predictAll(emptyTest).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "rates = test.map(lambda r: ((r[0], r[1]), r[2])).join(pred)\n",
    "end = time.time()\n",
    "print('Test completed for rank: ' + str(the_best_rank) + ' Elapsed time: ' + str(round(end - start, 2)) + ' seconds.')\n",
    "\n",
    "############################################################\n",
    "\n",
    "products = model.recommendProductsForUsers(10)\n",
    "products.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Fight Club: Bonus Material\" dla wartości 4.866043521149377\n",
      "\"Thomas & Sarah\" dla wartości 4.721042942324603\n",
      "\"The Element of Crime\" dla wartości 4.708888030404897\n",
      "\"Niea 7\" dla wartości 4.6854508074529715\n",
      "\"Tuck Everlasting\" dla wartości 4.6836380275702485\n",
      "\"Cinderella II\" dla wartości 4.674958529953608\n",
      "\"Creepshow\" dla wartości 4.671594230892056\n",
      "\"Scratch\" dla wartości 4.635454454397707\n",
      "\"Black Orpheus\" dla wartości 4.603810422278698\n",
      "\"King Cobra\" dla wartości 4.603664821979537\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "productions = model.productFeatures()\n",
    "productions_matrix = np.array(movies.map(lambda rv:rv[1]).collect())\n",
    "m2rn = productions.map(lambda rv:rv[0]).collect()\n",
    "\n",
    "############################################################\n",
    "\n",
    "print('Recommended movies: ')\n",
    "for i in enumerate(products.first()[1]):\n",
    "    product_index = i[1][1]\n",
    "    print(\"\\\"\" + id2title[m2rn.index(product_index)] + \"\\\" for value %s\" % i[1][2])\n",
    "\n",
    "id2title[1757]\n",
    "lotr_idx = m2rn.index(1757)\n",
    "lotr_vector = productions_matrix[lotr_idx]\n",
    "\n",
    "############################################################\n",
    "\n",
    "print('Title of watched movie: ' + id2title[1757])\n",
    "print('Vector of watched movie:')\n",
    "print(lotr_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 4\n",
    "Zrób program \"co obejrzeć dziś wieczorem\".\n",
    "Program powinien pokazaywać 5 tytułow, uzytkownik wskazuje jeden lub kilka z nich i system pokazuje kolejne rekomendacje. Każda kolejna rekomendacja powinna wskazywać coraz lepsze rekomendacje. \n",
    "\n",
    "problem można rozwiazać na kilka sposób. Można pogrupować przestrzeń filmów (macierz P.T) i pokazywać z różnych np. centroidy klastrów (które będą reprezentować gatunki filmów (prawdopodobnie) ). \n",
    "\n",
    "każde kolejne wybranie filmu będzie wymagało stworzenie nowego sztuznego użytkownika z ratingiem. W każdej iteracji takiego użytkownika należy rzucic na macierz Q (czyli na latentną macierz użytkownika). Przypatrz się dokładnie sposobowi mnożenia na obrazku powyżej i zastanów się jak otrzymać takiego użytkownika w przestrzeni cech?). Przypominam że macierz P.T jest nieodracalna ale można wykorzystać pseudo odwrotnosć."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
