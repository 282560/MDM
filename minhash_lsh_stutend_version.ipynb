{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinHASH LSH\n",
    "\n",
    "wyszukiwanie podobnych dokumentow i duplukatow uzywajac wyjątkowo małej reprezentacji dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importujemy pyspark oraz tworzyny nowy kontekst (reprezentujący połączenie z serwerem spark). W tym przypadku\n",
    "# serwer zostanie wystartowny automatycznie\n",
    "\n",
    "import pyspark \n",
    "import pyspark.sql as psql\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark_session = psql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszym krokiem jest tokenizacja [link](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html]). Zrobimy to na zabawkowym przykładzie ale z wykorzystaniem rozproszenia oferowanego przez spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Ala ma kota a kot...|\n",
      "|Hania ma kangura ...|\n",
      "|A to jest zupelni...|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                text|           tokenized|\n",
      "+--------------------+--------------------+\n",
      "|Ala ma kota a kot...|[Ala, ma, kota, a...|\n",
      "|Hania ma kangura ...|[Hania, ma, kangu...|\n",
      "|A to jest zupelni...|[A, to, jest, zup...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "Row(text='Ala ma kota a kot ma ale', tokenized='[Ala, ma, kota, a, kot, ma, ale]')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "# zbior dokumentow jako DataFrame a kazdy wiersz to bag-of-words\n",
    "\n",
    "raw_docs = spark_session.createDataFrame([\n",
    "    (\"Ala ma kota a kot ma ale\", ),\n",
    "    (\"Hania ma kangura ale nie ma kota\", ),\n",
    "    (\"A to jest zupelnie inne zdanie\", )\n",
    "], [\"text\"])\n",
    "\n",
    "raw_docs.show()\n",
    "\n",
    "tokenizer = udf(lambda text: text.split()) #stworzmy tokenizer i zdefinujmy go jako user-defined-function\n",
    "\n",
    "#nowa kolumna na podstawie \n",
    "tokenized_docs = raw_docs.withColumn('tokenized', tokenizer(col('text')))\n",
    "\n",
    "tokenized_docs.show()\n",
    "print (tokenized_docs.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1\n",
    "wykorzystaj gotowy tokenizer z biblioteki ml.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|           tokenized|\n",
      "+--------------------+--------------------+\n",
      "|Ala ma kota a kot...|[ala, ma, kota, a...|\n",
      "|Hania ma kangura ...|[hania, ma, kangu...|\n",
      "|A to jest zupelni...|[a, to, jest, zup...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.feature as feature\n",
    "\n",
    "tokenizer = feature.Tokenizer(inputCol=\"text\", outputCol=\"tokenized\")\n",
    "\n",
    "tokenized_docs = tokenizer.transform(raw_docs)\n",
    "\n",
    "tokenized_docs.show()\n",
    "\n",
    "# Przkład, na bazie którego się uczyłem (zakładka Python):\n",
    "# https://spark.apache.org/docs/latest//ml-features.html#tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak jak na zajęciach WDAM, pracowaliśmy z wektorową reprezentacją dokumentów [Vector-Space Model](https://en.wikipedia.org/wiki/Vector_space_model) tutaj zrobimy to samo przy uzyciu sparkowej biblioteki do uczenia maszynkowego (mllib), lecz zamiast znanego nam modelu TF-IDF uzyjemy skip-n-gram modelu [link](https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c) zaimplementowanego w obiekcie Word2Vec. Odsylam do dokumentacji [tutaj](https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zobaczmy jak wyglada wektorowa reprezentacja slow:\n",
      "{'nie': [-0.12096006, -0.07376015, 0.06414847, -0.054982137], 'kangura': [-0.092503026, -0.041992985, 0.05138157, 0.049292855], 'inne': [0.05887832, -0.07352105, 0.12002677, -0.04434246], 'jest': [-0.049551904, -0.09689385, -0.06582698, 0.103375256], 'a': [0.086889826, 0.080413036, 0.039353184, 0.055977847], 'zupelnie': [0.10638169, -0.092406146, 0.11710746, 0.015190171], 'ma': [-0.0801062, -0.12025679, 0.08287904, -0.084533386], 'to': [-0.02409116, -0.07899488, 0.09550898, 0.12307345], 'kota': [0.068966985, 0.07459682, 0.12357131, -0.038191374], 'ale': [-0.061938643, -0.08345687, 0.11839861, -0.054773048], 'kot': [-0.0077602034, -0.0011759367, 0.11759224, 0.0828641], 'ala': [0.10962323, -0.0016566998, 0.104222566, 0.008013899], 'hania': [0.096656516, 0.107498996, 0.11271782, 0.040471867], 'zdanie': [-0.07931474, 0.11996545, -0.00759946, 0.084347636]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseVector([0.1096, -0.0017, 0.1042, 0.008])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "rdd = sc.parallelize( [\n",
    "    \"Ala ma kota a kot ma ale\".lower().split(' '),\n",
    "    \"Hania ma kangura ale nie ma kota\".lower().split(' '), \n",
    "    \"A to jest zupelnie inne zdanie\".lower().split(' ') ])\n",
    "#mozemy uzyc pre-trenowanego modelu, ale w konteksice naszego problemu o wiele lepiej znalezc parametry modelu samemu\n",
    "model = Word2Vec().setVectorSize(4).setMinCount(1)\n",
    "model = model.fit(rdd)\n",
    "\n",
    "print ('Zobaczmy jak wyglada wektorowa reprezentacja slow:')\n",
    "print (model.getVectors())\n",
    "\n",
    "#reprezentacja slowa jest zwrona jako gestwy wektor wielkosci 3\n",
    "model.transform('ala')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W naszym zabawkowym przykladzie Vec2Model jest zbyt zlozonym modelem. Sprobjmy wykorzystac bardzo prosta reprezetancje wektorowa zliczajaca slowa w dokumencie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|           tokenized|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Ala ma kota a kot...|[ala, ma, kota, a...|(14,[0,1,2,3,5,12...|\n",
      "|Hania ma kangura ...|[hania, ma, kangu...|(14,[0,2,3,4,8,9]...|\n",
      "|A to jest zupelni...|[a, to, jest, zup...|(14,[1,6,7,10,11,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "Row(text='Ala ma kota a kot ma ale', tokenized=['ala', 'ma', 'kota', 'a', 'kot', 'ma', 'ale'], features=SparseVector(14, {0: 2.0, 1: 1.0, 2: 1.0, 3: 1.0, 5: 1.0, 12: 1.0}))\n",
      "['ma', 'a', 'kota', 'ale', 'nie', 'ala', 'zupelnie', 'to', 'kangura', 'hania', 'zdanie', 'inne', 'kot', 'jest']\n",
      "\n",
      "Pierwszy dokument ma nastepujace zakodowane (w postaci rzadkiej reprezentacji) slowa\n",
      "(14,[0,1,2,3,5,12],[2.0,1.0,1.0,1.0,1.0,1.0])\n",
      "to jest obiekt sparseVector, ktory reprezentuje wiersz rzadkiej macierzy o 14-tu kolumnach z wartosciami niezerowymi w [0,1,2,3,6,8]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv_model = CountVectorizer(inputCol='tokenized', outputCol='features', \n",
    "                            minDF=1.0, vocabSize=100)\n",
    "cv_model = cv_model.fit(tokenized_docs)\n",
    "features_df = cv_model.transform(tokenized_docs)\n",
    "\n",
    "features_df.show()\n",
    "\n",
    "print (features_df.head())\n",
    "print (cv_model.vocabulary)\n",
    "print ('\\nPierwszy dokument ma nastepujace zakodowane (w postaci rzadkiej reprezentacji) slowa')\n",
    "print (features_df.first()['features'])\n",
    "print ('to jest obiekt sparseVector, ktory reprezentuje wiersz rzadkiej macierzy o 14-tu kolumnach z wartosciami niezerowymi w [0,1,2,3,6,8]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|            datasetA|            datasetB|   JaccardDistance|\n",
      "+--------------------+--------------------+------------------+\n",
      "|[Ala ma kota a ko...|[Ala ma kota a ko...|               0.0|\n",
      "|[Hania ma kangura...|[Hania ma kangura...|               0.0|\n",
      "|[Ala ma kota a ko...|[Hania ma kangura...|0.6666666666666667|\n",
      "|[Hania ma kangura...|[Ala ma kota a ko...|0.6666666666666667|\n",
      "|[A to jest zupeln...|[A to jest zupeln...|               0.0|\n",
      "+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "\n",
    "mh_model = MinHashLSH(inputCol='features', outputCol='hashes', \n",
    "                      numHashTables=2)\n",
    "\n",
    "#mh_model = MinHashLSH(inputCol='features', outputCol='hashes', numHashTables=6) #\"startowo\"\n",
    "\n",
    "mh_model = mh_model.fit(features_df)\n",
    "\n",
    "similarity = mh_model.approxSimilarityJoin(features_df, features_df, 1, distCol='JaccardDistance')\n",
    "\n",
    "similarity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2\n",
    "Jaka jest minimalna ilosc fukcji haszujacych aby wychwycic podobienstow 0.666(6) miedzy pierwszym a drugim dokumentem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Minimalna ilość fukcji haszujacych, aby\n",
    "wychwycić podobieństwo 0.(6) między pierwszym,\n",
    "a drugim dokumentem to: numHashTables=2.\n",
    "    \n",
    "Dla numHashTables=1 JaccardDistance daje\n",
    "jeszcze wszędzie 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3\n",
    "wykorzystaj mh_model.approxNearestNeighbors aby znalezc 'podobny' dokument do new_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|           tokenized|\n",
      "+--------------------+--------------------+\n",
      "|Ala ma psa a nie ...|[ala, ma, psa, a,...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "dokument:Ala ma kota a kot ma ale, podobienstwo = 0.4285714285714286\n",
      "dokument:Hania ma kangura ale nie ma kota, podobienstwo = 0.625\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.feature as feature\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "new_doc = \"Ala ma psa a nie ma kota\"\n",
    "\n",
    "new_doc_df = spark_session.createDataFrame([(new_doc, )], [\"text\"])\n",
    "\n",
    "tokenizer_new = feature.Tokenizer(inputCol=\"text\", outputCol=\"tokenized\")\n",
    "tokenized_docs_new = tokenizer_new.transform(new_doc_df)\n",
    "tokenized_docs_new.show()\n",
    "\n",
    "new_doc_features = cv_model.transform(tokenized_docs_new)\n",
    "\n",
    "ret = mh_model.approxNearestNeighbors(features_df, new_doc_features.first()['features'],  5)\n",
    "\n",
    "for row in ret.collect():\n",
    "    print ('Dokument: ' + row['text'] +', podobieśstwo: '+ str(row['distCol']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 4 (duże)\n",
    "\n",
    "Pobierz zbior danych z rosyjskiej gazety pravda [link](http://fizyka.umk.pl/~mich/pravda_extracted.tar.gz) zebranej podczas ataku Rosji na Ukraine. \n",
    "* wczytaj te dokumenty do DataFrame\n",
    "* zaaplikuj tokenizacje (sprobuje RegexTokenizer)\n",
    "* usun duplikaty uzywajac MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
