{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinHASH LSH\n",
    "\n",
    "wyszukiwanie podobnych dokumentow i duplukatow uzywajac wyjątkowo małej reprezentacji dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importujemy pyspark oraz tworzyny nowy kontekst (reprezentujący połączenie z serwerem spark). W tym przypadku\n",
    "# serwer zostanie wystartowny automatycznie\n",
    "\n",
    "import pyspark \n",
    "import pyspark.sql as psql\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark_session = psql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszym krokiem jest tokenizacja [link](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html]). Zrobimy to na zabawkowym przykładzie ale z wykorzystaniem rozproszenia oferowanego przez spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Ala ma kota a kot...|\n",
      "|Hania ma kangura ...|\n",
      "|A to jest zupelni...|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                text|           tokenized|\n",
      "+--------------------+--------------------+\n",
      "|Ala ma kota a kot...|[Ala, ma, kota, a...|\n",
      "|Hania ma kangura ...|[Hania, ma, kangu...|\n",
      "|A to jest zupelni...|[A, to, jest, zup...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "Row(text='Ala ma kota a kot ma ale', tokenized='[Ala, ma, kota, a, kot, ma, ale]')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "# zbior dokumentow jako DataFrame a kazdy wiersz to bag-of-words\n",
    "\n",
    "raw_docs = spark_session.createDataFrame([\n",
    "    (\"Ala ma kota a kot ma ale\", ),\n",
    "    (\"Hania ma kangura ale nie ma kota\", ),\n",
    "    (\"A to jest zupelnie inne zdanie\", )\n",
    "], [\"text\"])\n",
    "\n",
    "raw_docs.show()\n",
    "\n",
    "tokenizer = udf(lambda text: text.split()) #stworzmy tokenizer i zdefinujmy go jako user-defined-function\n",
    "\n",
    "#nowa kolumna na podstawie \n",
    "tokenized_docs = raw_docs.withColumn('tokenized', tokenizer(col('text')))\n",
    "\n",
    "tokenized_docs.show()\n",
    "print (tokenized_docs.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1\n",
    "wykorzystaj gotowy tokenizer z biblioteki ml.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|           tokenized|\n",
      "+--------------------+--------------------+\n",
      "|Ala ma kota a kot...|[ala, ma, kota, a...|\n",
      "|Hania ma kangura ...|[hania, ma, kangu...|\n",
      "|A to jest zupelni...|[a, to, jest, zup...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.feature as feature\n",
    "\n",
    "tokenizer = feature.Tokenizer(inputCol=\"text\", outputCol=\"tokenized\")\n",
    "\n",
    "tokenized_docs = tokenizer.transform(raw_docs)\n",
    "\n",
    "tokenized_docs.show()\n",
    "\n",
    "# Przkład, na bazie którego się uczyłem (zakładka Python):\n",
    "# https://spark.apache.org/docs/latest//ml-features.html#tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak jak na zajęciach WDAM, pracowaliśmy z wektorową reprezentacją dokumentów [Vector-Space Model](https://en.wikipedia.org/wiki/Vector_space_model) tutaj zrobimy to samo przy uzyciu sparkowej biblioteki do uczenia maszynkowego (mllib), lecz zamiast znanego nam modelu TF-IDF uzyjemy skip-n-gram modelu [link](https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c) zaimplementowanego w obiekcie Word2Vec. Odsylam do dokumentacji [tutaj](https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zobaczmy jak wyglada wektorowa reprezentacja slow:\n",
      "{'nie': [0.114528336, -0.006846724, -0.025199931, 0.08501015], 'kangura': [0.063611336, 0.025590949, 0.029352859, 0.0075537614], 'inne': [-0.07924052, -0.121790625, -0.09923935, 0.12075767], 'jest': [-0.043344904, -0.046249412, -0.123785876, 0.09940506], 'a': [0.02048348, 0.03422871, -0.097975075, 0.092381984], 'zupelnie': [0.05157309, 0.06834473, 0.07105093, -0.040504053], 'ma': [0.11533597, -0.054423075, -0.04744574, 0.049611572], 'to': [-0.11152293, 0.084827356, -0.022310827, 0.07881487], 'kota': [-0.07808207, -0.0019579693, 0.123322286, 0.1223986], 'ale': [0.061333854, 0.047559403, 0.006485404, -0.053311795], 'kot': [-0.07221441, -0.016918432, 0.029325038, -0.109216824], 'ala': [-0.00823792, -0.054052386, 0.084926665, 0.028311351], 'hania': [-0.08199044, 0.017508304, 0.00076313416, 0.0066684536], 'zdanie': [0.04390236, 0.08001853, -0.036443405, -0.03994095]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.0082, -0.0541, 0.0849, 0.0283])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "rdd = sc.parallelize( [\n",
    "    \"Ala ma kota a kot ma ale\".lower().split(' '),\n",
    "    \"Hania ma kangura ale nie ma kota\".lower().split(' '), \n",
    "    \"A to jest zupelnie inne zdanie\".lower().split(' ') ])\n",
    "#mozemy uzyc pre-trenowanego modelu, ale w konteksice naszego problemu o wiele lepiej znalezc parametry modelu samemu\n",
    "model = Word2Vec().setVectorSize(4).setMinCount(1)\n",
    "model = model.fit(rdd)\n",
    "\n",
    "print ('Zobaczmy jak wyglada wektorowa reprezentacja slow:')\n",
    "print (model.getVectors())\n",
    "\n",
    "#reprezentacja slowa jest zwrona jako gestwy wektor wielkosci 3\n",
    "model.transform('ala')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W naszym zabawkowym przykladzie Vec2Model jest zbyt zlozonym modelem. Sprobjmy wykorzystac bardzo prosta reprezetancje wektorowa zliczajaca slowa w dokumencie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|           tokenized|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Ala ma kota a kot...|[ala, ma, kota, a...|(14,[0,1,2,3,5,9]...|\n",
      "|Hania ma kangura ...|[hania, ma, kangu...|(14,[0,1,3,4,7,11...|\n",
      "|A to jest zupelni...|[a, to, jest, zup...|(14,[2,6,8,10,12,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "Row(text='Ala ma kota a kot ma ale', tokenized=['ala', 'ma', 'kota', 'a', 'kot', 'ma', 'ale'], features=SparseVector(14, {0: 2.0, 1: 1.0, 2: 1.0, 3: 1.0, 5: 1.0, 9: 1.0}))\n",
      "['ma', 'ale', 'a', 'kota', 'nie', 'ala', 'zupelnie', 'kangura', 'inne', 'kot', 'jest', 'hania', 'to', 'zdanie']\n",
      "\n",
      "Pierwszy dokument ma nastepujace zakodowane (w postaci rzadkiej reprezentacji) slowa\n",
      "(14,[0,1,2,3,5,9],[2.0,1.0,1.0,1.0,1.0,1.0])\n",
      "to jest obiekt sparseVector, ktory reprezentuje wiersz rzadkiej macierzy o 14-tu kolumnach z wartosciami niezerowymi w [0,1,2,3,6,8]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv_model = CountVectorizer(inputCol='tokenized', outputCol='features', \n",
    "                            minDF=1.0, vocabSize=100)\n",
    "cv_model = cv_model.fit(tokenized_docs)\n",
    "features_df = cv_model.transform(tokenized_docs)\n",
    "\n",
    "features_df.show()\n",
    "\n",
    "print (features_df.head())\n",
    "print (cv_model.vocabulary)\n",
    "print ('\\nPierwszy dokument ma nastepujace zakodowane (w postaci rzadkiej reprezentacji) slowa')\n",
    "print (features_df.first()['features'])\n",
    "print ('to jest obiekt sparseVector, ktory reprezentuje wiersz rzadkiej macierzy o 14-tu kolumnach z wartosciami niezerowymi w [0,1,2,3,6,8]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|            datasetA|            datasetB|   JaccardDistance|\n",
      "+--------------------+--------------------+------------------+\n",
      "|[Ala ma kota a ko...|[Hania ma kangura...|0.6666666666666667|\n",
      "|[Hania ma kangura...|[Ala ma kota a ko...|0.6666666666666667|\n",
      "|[A to jest zupeln...|[A to jest zupeln...|               0.0|\n",
      "|[Ala ma kota a ko...|[Ala ma kota a ko...|               0.0|\n",
      "|[Hania ma kangura...|[Hania ma kangura...|               0.0|\n",
      "+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "\n",
    "mh_model = MinHashLSH(inputCol='features', outputCol='hashes', \n",
    "                      numHashTables=2)\n",
    "\n",
    "#mh_model = MinHashLSH(inputCol='features', outputCol='hashes', numHashTables=6) #\"startowo\"\n",
    "\n",
    "mh_model = mh_model.fit(features_df)\n",
    "\n",
    "similarity = mh_model.approxSimilarityJoin(features_df, features_df, 1, distCol='JaccardDistance')\n",
    "\n",
    "similarity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2\n",
    "Jaka jest minimalna ilosc fukcji haszujacych aby wychwycic podobienstow 0.666(6) miedzy pierwszym a drugim dokumentem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Minimalna ilość fukcji haszujacych, aby\n",
    "wychwycić podobieństwo 0.(6) między pierwszym,\n",
    "a drugim dokumentem to: numHashTables=2.\n",
    "    \n",
    "Dla numHashTables=1 JaccardDistance daje\n",
    "jeszcze wszędzie 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3\n",
    "wykorzystaj mh_model.approxNearestNeighbors aby znalezc 'podobny' dokument do new_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|           tokenized|\n",
      "+--------------------+--------------------+\n",
      "|Ala ma psa a nie ...|[ala, ma, psa, a,...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "Dokument: Ala ma kota a kot ma ale, podobieństwo: 0.4285714285714286\n",
      "Dokument: Hania ma kangura ale nie ma kota, podobieństwo: 0.625\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.feature as feature\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "new_doc = \"Ala ma psa a nie ma kota\"\n",
    "\n",
    "new_doc_df = spark_session.createDataFrame([(new_doc, )], [\"text\"])\n",
    "\n",
    "tokenizer_new = feature.Tokenizer(inputCol=\"text\", outputCol=\"tokenized\")\n",
    "tokenized_docs_new = tokenizer_new.transform(new_doc_df)\n",
    "tokenized_docs_new.show()\n",
    "\n",
    "new_doc_features = cv_model.transform(tokenized_docs_new)\n",
    "\n",
    "ret = mh_model.approxNearestNeighbors(features_df, new_doc_features.first()['features'],  5)\n",
    "\n",
    "for row in ret.collect():\n",
    "    print ('Dokument: ' + row['text'] +', podobieństwo: '+ str(row['distCol']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 4 (duże)\n",
    "\n",
    "Pobierz zbior danych z rosyjskiej gazety pravda [link](http://fizyka.umk.pl/~mich/pravda_extracted.tar.gz) zebranej podczas ataku Rosji na Ukraine. \n",
    "* wczytaj te dokumenty do DataFrame\n",
    "* zaaplikuj tokenizacje (sprobuje RegexTokenizer)\n",
    "* usun duplikaty uzywajac MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
