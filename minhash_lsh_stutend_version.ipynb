{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinHASH LSH\n",
    "\n",
    "wyszukiwanie podobnych dokumentow i duplukatow uzywajac wyjątkowo małej reprezentacji dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importujemy pyspark oraz tworzyny nowy kontekst (reprezentujący połączenie z serwerem spark).\n",
    "# W tym przypadku serwer zostanie wystartowny automatycznie.\n",
    "\n",
    "import pyspark \n",
    "import pyspark.sql as psql\n",
    "\n",
    "sc = pyspark.SparkContext() # SparkContext to Entry Point do jakiejkolwiek funkcjonalności spark'a.\n",
    "spark_session = psql.SparkSession(sc) # Główny Entry Point do DataFrame i funkcjonalności SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszym krokiem jest tokenizacja [link](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html]). Zrobimy to na zabawkowym przykładzie ale z wykorzystaniem rozproszenia oferowanego przez spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Ala ma kota a kot...|\n",
      "|Hania ma kangura ...|\n",
      "|A to jest zupelni...|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                text|           tokenized|\n",
      "+--------------------+--------------------+\n",
      "|Ala ma kota a kot...|[Ala, ma, kota, a...|\n",
      "|Hania ma kangura ...|[Hania, ma, kangu...|\n",
      "|A to jest zupelni...|[A, to, jest, zup...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "Row(text='Ala ma kota a kot ma ale', tokenized='[Ala, ma, kota, a, kot, ma, ale]')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col # User-defined functions | Column expression\n",
    "\n",
    "# Zbiór dokumentów jako DataFrame, a każdy wiersz to bag-of-words\n",
    "\n",
    "raw_docs = spark_session.createDataFrame([\n",
    "    (\"Ala ma kota a kot ma ale\", ),\n",
    "    (\"Hania ma kangura ale nie ma kota\", ),\n",
    "    (\"A to jest zupelnie inne zdanie\", )\n",
    "], [\"text\"])\n",
    "\n",
    "raw_docs.show()\n",
    "\n",
    "tokenizer = udf(lambda text: text.split()) # Stwórzmy tokenizer i zdefinujmy go jako User-defined function\n",
    "\n",
    "# Nowa kolumna na podstawie \n",
    "tokenized_docs = raw_docs.withColumn('tokenized', tokenizer(col('text')))\n",
    "\n",
    "tokenized_docs.show()\n",
    "print (tokenized_docs.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1\n",
    "wykorzystaj gotowy tokenizer z biblioteki ml.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|           tokenized|\n",
      "+--------------------+--------------------+\n",
      "|Ala ma kota a kot...|[ala, ma, kota, a...|\n",
      "|Hania ma kangura ...|[hania, ma, kangu...|\n",
      "|A to jest zupelni...|[a, to, jest, zup...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.feature as feature\n",
    "\n",
    "tokenizer = feature.Tokenizer(inputCol=\"text\", outputCol=\"tokenized\")\n",
    "# Tokenizacja to rozdzielanie tekstu na poszczególne terminy (słowa)\n",
    "\n",
    "tokenized_docs = tokenizer.transform(raw_docs)\n",
    "\n",
    "tokenized_docs.show()\n",
    "\n",
    "# Przkład, na bazie którego się uczyłem (zakładka Python):\n",
    "# https://spark.apache.org/docs/latest//ml-features.html#tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak jak na zajęciach WDAM, pracowaliśmy z wektorową reprezentacją dokumentów [Vector-Space Model](https://en.wikipedia.org/wiki/Vector_space_model) tutaj zrobimy to samo przy uzyciu sparkowej biblioteki do uczenia maszynkowego (mllib), lecz zamiast znanego nam modelu TF-IDF ( _Term frequency-inverse document frequency_ ) uzyjemy skip-n-gram modelu [link](https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c) zaimplementowanego w obiekcie Word2Vec (Word2Vec to estymator, który bierze sekwencje słów reprezentujących dokumenty i trenuje Word2VecModel). Odsylam do dokumentacji [tutaj](https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zobaczmy jak wyglada wektorowa reprezentacja slow:\n",
      "{'nie': [0.011366038, -0.061218515, 0.07828797, 0.115417056], 'kangura': [0.055424206, -0.06974186, -0.057689942, -0.10136634], 'inne': [-0.012856989, 0.010159543, -0.020994551, 0.075678974], 'jest': [0.00023627638, 0.089690216, -0.024828862, 0.05418939], 'a': [-0.0904789, 0.017946294, -0.012714515, 0.049510144], 'zupelnie': [0.07643746, -0.03218023, -0.1252622, 0.04103435], 'ma': [-0.031219292, 0.10784219, -0.031639963, 0.053703185], 'to': [-0.059488174, -0.021263372, -0.08151385, 0.056854296], 'kota': [0.11185704, -0.093370475, 0.11293637, -0.116649106], 'ale': [0.10859567, 0.10285717, 0.046242457, -0.097419985], 'kot': [-0.113378964, 0.091064535, 0.03890799, 0.076091126], 'ala': [-0.025984691, 0.07186547, 0.041707955, -0.011838384], 'hania': [-0.042351246, 0.03937711, 0.040577114, -0.12101654], 'zdanie': [0.10388712, 0.088408604, -0.11458644, -0.1036376]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.026, 0.0719, 0.0417, -0.0118])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "# Stworzenie Resilient Distributed Datasets\n",
    "rdd = sc.parallelize( [\n",
    "    \"Ala ma kota a kot ma ale\".lower().split(' '),\n",
    "    \"Hania ma kangura ale nie ma kota\".lower().split(' '), \n",
    "    \"A to jest zupelnie inne zdanie\".lower().split(' ') ])\n",
    "# Możemy użyć pre-trenowanego modelu, ale w kontekście naszego problemu o wiele lepiej znaleźć parametry modelu samemu\n",
    "model = Word2Vec().setVectorSize(4).setMinCount(1)\n",
    "model = model.fit(rdd)\n",
    "\n",
    "print ('Zobaczmy jak wyglada wektorowa reprezentacja slow:')\n",
    "print (model.getVectors())\n",
    "\n",
    "#Skip-n-Gram - Skip-gram to jedna z technik uczenia nienadzorowanego, wykorzystywanych do znajdowania\n",
    "    # słów najbardziej powiązanych z danym słowem.\n",
    "\n",
    "#reprezentacja slowa jest zwrona jako gestwy wektor wielkosci 3\n",
    "model.transform('ala')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W naszym zabawkowym przykladzie Vec2Model jest zbyt zlozonym modelem. Sprobjmy wykorzystac bardzo prosta reprezetancje wektorowa zliczajaca slowa w dokumencie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|           tokenized|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Ala ma kota a kot...|[ala, ma, kota, a...|(14,[0,1,2,3,7,11...|\n",
      "|Hania ma kangura ...|[hania, ma, kangu...|(14,[0,1,3,9,10,1...|\n",
      "|A to jest zupelni...|[a, to, jest, zup...|(14,[2,4,5,6,8,12...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "Row(text='Ala ma kota a kot ma ale', tokenized=['ala', 'ma', 'kota', 'a', 'kot', 'ma', 'ale'], features=SparseVector(14, {0: 2.0, 1: 1.0, 2: 1.0, 3: 1.0, 7: 1.0, 11: 1.0}))\n",
      "['ma', 'ale', 'a', 'kota', 'to', 'zdanie', 'inne', 'kot', 'jest', 'hania', 'nie', 'ala', 'zupelnie', 'kangura']\n",
      "\n",
      "Pierwszy dokument ma nastepujace zakodowane (w postaci rzadkiej reprezentacji) slowa\n",
      "(14,[0,1,2,3,7,11],[2.0,1.0,1.0,1.0,1.0,1.0])\n",
      "to jest obiekt sparseVector, ktory reprezentuje wiersz rzadkiej macierzy o 14-tu kolumnach z wartosciami niezerowymi w [0,1,2,3,6,8]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv_model = CountVectorizer(inputCol='tokenized', outputCol='features', \n",
    "                            minDF=1.0, vocabSize=100)\n",
    "cv_model = cv_model.fit(tokenized_docs)\n",
    "features_df = cv_model.transform(tokenized_docs)\n",
    "\n",
    "features_df.show()\n",
    "\n",
    "print (features_df.head())\n",
    "print (cv_model.vocabulary)\n",
    "print ('\\nPierwszy dokument ma nastepujace zakodowane (w postaci rzadkiej reprezentacji) slowa')\n",
    "print (features_df.first()['features'])\n",
    "print ('to jest obiekt sparseVector, ktory reprezentuje wiersz rzadkiej macierzy o 14-tu kolumnach z wartosciami niezerowymi w [0,1,2,3,6,8]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|            datasetA|            datasetB|   JaccardDistance|\n",
      "+--------------------+--------------------+------------------+\n",
      "|[A to jest zupeln...|[A to jest zupeln...|               0.0|\n",
      "|[Hania ma kangura...|[Ala ma kota a ko...|0.6666666666666667|\n",
      "|[Ala ma kota a ko...|[Hania ma kangura...|0.6666666666666667|\n",
      "|[Ala ma kota a ko...|[Ala ma kota a ko...|               0.0|\n",
      "|[Hania ma kangura...|[Hania ma kangura...|               0.0|\n",
      "+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "\n",
    "mh_model = MinHashLSH(inputCol='features', outputCol='hashes', \n",
    "                      numHashTables=2)\n",
    "\n",
    "#mh_model = MinHashLSH(inputCol='features', outputCol='hashes', numHashTables=6) #\"startowo\"\n",
    "\n",
    "mh_model = mh_model.fit(features_df)\n",
    "\n",
    "similarity = mh_model.approxSimilarityJoin(features_df, features_df, 1, distCol='JaccardDistance')\n",
    "\n",
    "similarity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2\n",
    "Jaka jest minimalna ilosc fukcji haszujacych aby wychwycic podobienstow 0.666(6) miedzy pierwszym a drugim dokumentem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Minimalna ilość fukcji haszujacych, aby\n",
    "wychwycić podobieństwo 0.(6) między pierwszym,\n",
    "a drugim dokumentem to: numHashTables=2.\n",
    "    \n",
    "Dla numHashTables=1 JaccardDistance daje\n",
    "jeszcze wszędzie 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3\n",
    "wykorzystaj mh_model.approxNearestNeighbors aby znalezc 'podobny' dokument do new_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|           tokenized|\n",
      "+--------------------+--------------------+\n",
      "|Ala ma psa a nie ...|[ala, ma, psa, a,...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "Dokument: Ala ma kota a kot ma ale, podobieństwo: 0.4285714285714286\n",
      "Dokument: Hania ma kangura ale nie ma kota, podobieństwo: 0.625\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.feature as feature\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "new_doc = \"Ala ma psa a nie ma kota\"\n",
    "\n",
    "new_doc_df = spark_session.createDataFrame([(new_doc, )], [\"text\"])\n",
    "\n",
    "tokenizer_new = feature.Tokenizer(inputCol=\"text\", outputCol=\"tokenized\")\n",
    "tokenized_docs_new = tokenizer_new.transform(new_doc_df)\n",
    "tokenized_docs_new.show()\n",
    "\n",
    "new_doc_features = cv_model.transform(tokenized_docs_new)\n",
    "\n",
    "ret = mh_model.approxNearestNeighbors(features_df, new_doc_features.first()['features'],  5) # Przekazując duży\n",
    "    # zbiór danych i pojedynczy element, znajduje w przybliżeniu co najwyżej k elementów, które mają najbliższą odległość\n",
    "    # od elementu.\n",
    "\n",
    "for row in ret.collect():\n",
    "    print ('Dokument: ' + row['text'] +', podobieństwo: '+ str(row['distCol']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 4 (duże)\n",
    "\n",
    "Pobierz zbior danych z rosyjskiej gazety pravda [link](http://fizyka.umk.pl/~mich/pravda_extracted.tar.gz) zebranej podczas ataku Rosji na Ukraine. \n",
    "* wczytaj te dokumenty do DataFrame\n",
    "* zaaplikuj tokenizacje (sprobuje RegexTokenizer)\n",
    "* usun duplikaty uzywajac MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizowany tekst:\n",
      "+--------------------+--------------------+\n",
      "|                text|           tokenized|\n",
      "+--------------------+--------------------+\n",
      "|Nizhny Novgorod p...|[nizhny, novgorod...|\n",
      "|Can Russia save U...|[can, russia, sav...|\n",
      "|Women look at eac...|[women, look, at,...|\n",
      "|Ukrainian border ...|[ukrainian, borde...|\n",
      "|Anzhi out of Euro...|[anzhi, out, of, ...|\n",
      "|Syria: Chaos, cor...|[syria, chaos, co...|\n",
      "|Delegation from L...|[delegation, from...|\n",
      "|Should zoo animal...|[should, zoo, ani...|\n",
      "|Europa League: So...|[europa, league, ...|\n",
      "|Yelena Isinbayeva...|[yelena, isinbaye...|\n",
      "|Astrakhan city ad...|[astrakhan, city,...|\n",
      "|The future of Mer...|[the, future, of,...|\n",
      "|Explosion occurs ...|[explosion, occur...|\n",
      "|Moscow region mou...|[moscow, region, ...|\n",
      "|Alive baby nearly...|[alive, baby, nea...|\n",
      "|Russian troops ta...|[russian, troops,...|\n",
      "|Ukraine's Yanukov...|[ukraine, s, yanu...|\n",
      "|Ukrainian parliam...|[ukrainian, parli...|\n",
      "|Army hazing start...|[army, hazing, st...|\n",
      "|Longest suspensio...|[longest, suspens...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CountVectorizer:\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                text|           tokenized|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Nizhny Novgorod p...|[nizhny, novgorod...|(28223,[0,1,2,3,4...|\n",
      "|Can Russia save U...|[can, russia, sav...|(28223,[0,1,2,3,4...|\n",
      "|Women look at eac...|[women, look, at,...|(28223,[0,1,2,3,4...|\n",
      "|Ukrainian border ...|[ukrainian, borde...|(28223,[0,1,3,4,5...|\n",
      "|Anzhi out of Euro...|[anzhi, out, of, ...|(28223,[0,1,2,3,4...|\n",
      "|Syria: Chaos, cor...|[syria, chaos, co...|(28223,[0,1,2,3,4...|\n",
      "|Delegation from L...|[delegation, from...|(28223,[0,1,2,3,4...|\n",
      "|Should zoo animal...|[should, zoo, ani...|(28223,[0,1,2,3,4...|\n",
      "|Europa League: So...|[europa, league, ...|(28223,[0,1,2,3,4...|\n",
      "|Yelena Isinbayeva...|[yelena, isinbaye...|(28223,[0,1,2,3,4...|\n",
      "|Astrakhan city ad...|[astrakhan, city,...|(28223,[0,1,2,3,4...|\n",
      "|The future of Mer...|[the, future, of,...|(28223,[0,1,2,3,4...|\n",
      "|Explosion occurs ...|[explosion, occur...|(28223,[0,1,2,3,4...|\n",
      "|Moscow region mou...|[moscow, region, ...|(28223,[0,1,2,3,4...|\n",
      "|Alive baby nearly...|[alive, baby, nea...|(28223,[0,1,2,3,4...|\n",
      "|Russian troops ta...|[russian, troops,...|(28223,[0,1,2,3,4...|\n",
      "|Ukraine's Yanukov...|[ukraine, s, yanu...|(28223,[0,1,2,3,4...|\n",
      "|Ukrainian parliam...|[ukrainian, parli...|(28223,[0,1,2,3,4...|\n",
      "|Army hazing start...|[army, hazing, st...|(28223,[0,1,2,3,4...|\n",
      "|Longest suspensio...|[longest, suspens...|(28223,[0,1,2,3,4...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Zbiór bez duplikatów:\n",
      "+--------------------+--------------------+-------------------+\n",
      "|            datasetA|            datasetB|    JaccardDistance|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|[Champions League...|[Europa League: A...|0.07100591715976334|\n",
      "|[Europa League: A...|[Champions League...|0.07100591715976334|\n",
      "|[Champions League...|[Champions League...|0.09411764705882353|\n",
      "|[Champions League...|[Champions League...|0.09411764705882353|\n",
      "|[Champions: 4 Gam...|[Champions: Away ...|0.12413793103448278|\n",
      "|[Champions: Away ...|[Champions: 4 Gam...|0.12413793103448278|\n",
      "|[Anzhi out of Eur...|[Champions League...|0.12528473804100226|\n",
      "|[Champions League...|[Anzhi out of Eur...|0.12528473804100226|\n",
      "|[Champions: 4 Gam...|[Europa League: D...|0.14603174603174607|\n",
      "|[Europa League: D...|[Champions: 4 Gam...|0.14603174603174607|\n",
      "|[Champions League...|[Champions League...|0.14939024390243905|\n",
      "|[Champions League...|[Champions League...|0.14939024390243905|\n",
      "|[Champions League...|[Champions League...|0.15700483091787443|\n",
      "|[Champions League...|[Champions League...|0.15700483091787443|\n",
      "|[Champions League...|[Champions League...|0.16800000000000004|\n",
      "|[Champions League...|[Champions League...|0.16800000000000004|\n",
      "|[Europa League: A...|[Champions League...|0.17150395778364114|\n",
      "|[Champions League...|[Europa League: A...|0.17150395778364114|\n",
      "|[Soccer: Going in...|[Soccer: How they...| 0.1719298245614035|\n",
      "|[Soccer: How they...|[Soccer: Going in...| 0.1719298245614035|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Koniec!\n"
     ]
    }
   ],
   "source": [
    "# Do tokenizacji:\n",
    "import pyspark.ml.feature as feature\n",
    "# Do CountVectorizer:\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "# Do MinHASH:\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "\n",
    "# Wczytanie danych:\n",
    "content = sc.wholeTextFiles(\"./pravda_extracted/*\")\n",
    "\n",
    "# Utworzenie struktury:\n",
    "structure = content.collect()\n",
    "unzipped = []\n",
    "for entry in structure:\n",
    "    unzipped.append([entry[1]])\n",
    "\n",
    "# Wykorzystanie DataFrame (podpunkt 1):\n",
    "raw = spark_session.createDataFrame(unzipped, [\"text\"])\n",
    "\n",
    "# Tokenizacja (podpunkt 2):\n",
    "tokenizer = feature.RegexTokenizer(inputCol=\"text\",\n",
    "                                   outputCol=\"tokenized\",\n",
    "                                   pattern=\"\\\\W\")\n",
    "tokenized = tokenizer.transform(raw)\n",
    "print('Tokenizowany tekst:')\n",
    "tokenized.show()\n",
    "\n",
    "# CountVectorizer i CountVectorizerModel mają na celu pomóc w konwersji kolekcji dokumentów tekstowych na\n",
    "    # wektory liczby tokenów.\n",
    "\n",
    "# CountVectorizer:\n",
    "model = CountVectorizer(inputCol='tokenized',\n",
    "                        outputCol='features')\n",
    "model = model.fit(tokenized)\n",
    "features_df = model.transform(tokenized)\n",
    "print('CountVectorizer:')\n",
    "features_df.show()\n",
    "\n",
    "# MinHASH:\n",
    "minhash_model = MinHashLSH(inputCol='features',\n",
    "                           outputCol='hashes',\n",
    "                           numHashTables=2)\n",
    "minhash_model = minhash_model.fit(features_df)\n",
    "ret = minhash_model.approxSimilarityJoin(features_df,\n",
    "                                         features_df,\n",
    "                                         threshold=1,\n",
    "                                         distCol='JaccardDistance')\n",
    "\n",
    "# Współczynnik Jaccarda mierzy podobieństwo między dwoma zbiorami i jest zdefiniowany jako iloraz mocy\n",
    "    # części wspólnej zbiorów i mocy sumy tych zbiorów.\n",
    "\n",
    "# Tworzenie zbioru bez duplikatów (podpunkt 3):\n",
    "clear_data = ret.where(\"JaccardDistance != 0\").orderBy(\"JaccardDistance\") # Zerowa odległość (takie same)\n",
    "print('Zbiór bez duplikatów:')\n",
    "clear_data.show()\n",
    "\n",
    "print('Koniec!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
